vectordb:
  - name: text-embedding-3-large 
    db_type: chroma
    client_type: persistent
    embedding_model: https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d52603e5c324ff32/embeddings?api-version=2023-05-15
    path: ${PROJECT_DIR}/resources/chroma
  - name: text-embedding-3-small
    db_type: chroma
    client_type: persistent
    embedding_model: https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dbc221ce0d62ac80/embeddings?api-version=2023-05-15
    path: ${PROJECT_DIR}/resources/chroma
  - name: text-embedding-ada-002
    db_type: chroma
    client_type: persistent
    embedding_model: https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/daa827d6c7f93173/embeddings?api-version=2023-05-15
    path: ${PROJECT_DIR}/resources/chroma
  - name: gemini
    db_type: chroma
    client_type: persistent
    embedding_model: https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d18aaf36daf07d18/models/gemini-embedding:predict
    path: ${PROJECT_DIR}/resources/chroma

node_lines:
  - node_line_name: pre_retrieve_node_line
    nodes:
      - node_type: query_expansion
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 10
          top_k: [2, 6]
          retrieval_modules:
            - module_type: bm25
              bm25_tokenizer: [porter_stemmer, space, gpt2]
            - module_type: vectordb
              vectordb: [text-embedding-3-large,text-embedding-3-small, text-embedding-ada-002, gemini]
              embedding_batch: 256 
        modules:
          - module_type: pass_query_expansion
          - module_type: hyde
            generator_module_type: sap_api
            llm: anthropic
            model: claude-4-sonnet
            max_token: [64, 128]
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d7bbb5f70888f250/invoke"
          - module_type: query_decompose
            generator_module_type: sap_api
            llm: anthropic
            model: claude-4-sonnet
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d7bbb5f70888f250/invoke"
          - module_type: multi_query_expansion
            generator_module_type: sap_api
            llm: anthropic
            model: claude-4-sonnet
            temperature: [0.0, 1.0]
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d7bbb5f70888f250/invoke"

  - node_line_name: retrieve_node_line
    nodes:
      - node_type: retrieval
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 10
        top_k: [2, 6]
        modules:
          - module_type: bm25
            bm25_tokenizer: [porter_stemmer, space, gpt2]
          - module_type: vectordb
            vectordb: [text-embedding-3-large,text-embedding-3-small, text-embedding-ada-002, gemini]
            embedding_batch: 256
      - node_type: passage_reranker
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 10
        top_k: [1, 4]
        modules:
          - module_type: pass_reranker
          - module_type: monot5
            model_name:
              - castorini/monot5-base-msmarco-10k
              - castorini/monot5-large-msmarco-10k
              - unicamp-dl/ptt5-base-en-pt-msmarco-100k-v2
              - unicamp-dl/mt5-base-mmarco-v1
          - module_type: upr
          - module_type: colbert_reranker
          - module_type: sentence_transformer_reranker
            model_name:
              - cross-encoder/ms-marco-MiniLM-L12-v2
              - cross-encoder/ms-marco-TinyBERT-L2-v2
              - cross-encoder/stsb-distilroberta-base
          - module_type: flag_embedding_reranker
            model_name:
              - BAAI/bge-reranker-large
              - BAAI/bge-reranker-base
          - module_type: flag_embedding_llm_reranker
            model_name:
              - BAAI/bge-reranker-v2-m3
              - BAAI/bge-reranker-v2-gemma
          - module_type: flashrank_reranker
            model:
              - ms-marco-MiniLM-L-12-v2
              - ms-marco-MultiBERT-L-12
              - rank-T5-flan
          - module_type: sap_api
            model_name: cohere-rerank-v3.5
            api-url: "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d76c86b03037abf4/v1/rerank"

      - node_type: passage_filter
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 5
        modules:
          - module_type: pass_passage_filter
          - module_type: percentile_cutoff
            percentile: [0.4, 0.9]
          - module_type: similarity_threshold_cutoff
            threshold: [0.45, 0.95]
          - module_type: similarity_percentile_cutoff
            percentile: [0.4, 0.9]

      - node_type: passage_compressor
        strategy:
          metrics: [retrieval_token_f1, retrieval_token_recall, retrieval_token_precision]
          speed_threshold: 10
        modules:
          - module_type: pass_compressor
          - module_type: lexrank
            compression_ratio: [0.3, 0.7]
            threshold: [0.05, 0.3]
            damping: [0.75, 0.9]
            max_iterations: [15, 40]
          - module_type: spacy
            compression_ratio: [0.3, 0.5]
            spacy_model: ["en_core_web_sm", "en_core_web_md","en_core_web_lg","en_core_web_trf"]
          - module_type: tree_summarize
            generator_module_type: sap_api
            llm: anthropic
            model: claude-4-sonnet
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d7bbb5f70888f250/invoke"
          - module_type: refine
            generator_module_type: sap_api
            llm: anthropic
            model: claude-4-sonnet
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d7bbb5f70888f250/invoke"
     
  - node_line_name: post_retrieve_node_line
    nodes:
      - node_type: prompt_maker
        strategy:
          metrics:
            - metric_name: bleu
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai
          speed_threshold: 10
          generator_modules:
            - module_type: vllm
              llm: TinyLlama/TinyLlama-1.1B-Chat-v1.0
              temperature: [0.1, 1.0]
              max_tokens: 512
        modules:
          - module_type: fstring
            prompt:
              - "Answer to given questions with the following passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"
              - "There is a passages related to user question. Please response carefully to the following question. \n\n Passage: {retrieved_contents} \n\n Question: {query} \n\n Answer the question. Think step by step."
              - "{retrieved_contents} \n\n Read the passage carefully, and answer this question. \n\n Question: {query} \n\n Answer the question. Be concise."
          - module_type: long_context_reorder
            prompt:
              - "Answer to given questions with the following passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"
              - "There is a passages related to user question. Please response carefully to the following question. \n\n Passage: {retrieved_contents} \n\n Question: {query} \n\n Answer the question. Think step by step."
              - "{retrieved_contents} \n\n Read the passage carefully, and answer this question. \n\n Question: {query} \n\n Answer the question. Be concise."
          - module_type: window_replacement
            prompt:
              - "Tell me something about the question: {query} \n\n {retrieved_contents}"
              - "Question: {query} \n Something to read: {retrieved_contents} \n What's your answer?"
              - "Question: {query} \n\n Related information: \n{retrieved_contents} \n\n What would be a good response?"

      - node_type: generator
        strategy:
          metrics:
            - metric_name: bleu
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai
          speed_threshold: 10
        modules:
          - module_type: sap_api
            llm: mistralai
            model: [mistralai-large-instruct]
            temperature: [0.0, 1.0]
            max_token: 512
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d2c244844b703dc4/chat/completions"
          - module_type: sap_api
            llm: openai
            model: [gpt-3.5-turbo]
            temperature: [0.0, 1.0]
            max_token: 512
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d2410e85982024c2/chat/completions?api-version=2023-05-15"
          - module_type: sap_api
            llm: gemini
            model: Gemini-2.0-flash
            temperature: [0.0, 1.0]
            max_token: 512
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dda1e1c80422c6a8/models/gemini-2.0-flash:generateContent"
          - module_type: sap_api
            llm: anthropic
            model: claude-4-sonnet
            temperature: [0.0, 1.0]
            max_token: 512
            api_url: "https://api.ai.internalprod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d7bbb5f70888f250/invoke"

        
   
