vectordb:
  - name: bge_small
    db_type: chroma
    client_type: persistent
    embedding_model: huggingface_baai_bge_small
    collection_name: huggingface_baai_bge_small
    path: ${PROJECT_DIR}/resources/chroma
  - name: rubert
    db_type: chroma
    client_type: persistent
    embedding_model: huggingface_cointegrated_rubert_tiny2
    collection_name: huggingface_cointegrated_rubert_tiny2
    path: ${PROJECT_DIR}/resources/chroma
  - name: mpnet
    db_type: chroma
    client_type: persistent
    embedding_model: huggingface_all_mpnet_base_v2
    collection_name: huggingface_all_mpnet_base_v2
    path: ${PROJECT_DIR}/resources/chroma
  - name: bge_m3
    db_type: chroma
    client_type: persistent
    embedding_model: huggingface_bge_m3
    collection_name: huggingface_bge_m3
    path: ${PROJECT_DIR}/resources/chroma
node_lines:
  - node_line_name: pre_retrieve_node_line
    nodes:
      - node_type: query_expansion
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 10
          top_k: [2, 4]
          retrieval_modules:
            - module_type: bm25
              bm25_tokenizer: [porter_stemmer, space, gpt2]
            - module_type: vectordb
              vectordb: [bge_small, rubert, mpnet, bge_m3]
              embedding_batch: 256
        modules:
          - module_type: pass_query_expansion
          - module_type: query_decompose
            generator_module_type: vllm
            model: ["Qwen/Qwen2.5-1.5B-Instruct"]
          - module_type: hyde
            generator_module_type: vllm
            model: ["Qwen/Qwen2.5-1.5B-Instruct"]
            max_token: [64, 128]
          - module_type: multi_query_expansion
            generator_module_type: vllm
            model: ["Qwen/Qwen2.5-1.5B-Instruct"]
            temperature: [0.2, 1.0]
  - node_line_name: retrieve_node_line
    nodes:
      - node_type: retrieval
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 10
        top_k: [2, 4]
        modules:
          - module_type: bm25
            bm25_tokenizer: [porter_stemmer, space, gpt2]
          - module_type: vectordb
            vectordb: [bge_small, rubert, mpnet, bge_m3]
            embedding_batch: 256
      - node_type: passage_reranker
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 10
        top_k: [1, 3]
        modules:
          - module_type: pass_reranker
          - module_type: monot5
            model_name: [castorini/monot5-base-msmarco-10k, castorini/monot5-large-msmarco-10k, unicamp-dl/ptt5-base-en-pt-msmarco-100k-v2, unicamp-dl/mt5-base-mmarco-v1]
          - module_type: upr
          - module_type: colbert_reranker
          - module_type: sentence_transformer_reranker
            model_name: [cross-encoder/ms-marco-MiniLM-L12-v2, cross-encoder/ms-marco-TinyBERT-L2-v2, cross-encoder/stsb-distilroberta-base]
          - module_type: flag_embedding_reranker
            model_name: [BAAI/bge-reranker-large, BAAI/bge-reranker-base]
          - module_type: flag_embedding_llm_reranker
            model_name: [BAAI/bge-reranker-v2-gemma, BAAI/bge-reranker-v2-m3]
          - module_type: flashrank_reranker
            model: ["ms-marco-MiniLM-L-12-v2", "ms-marco-MultiBERT-L-12", "rank-T5-flan", "ce-esci-MiniLM-L12-v2"]
      - node_type: passage_filter
        strategy:
          metrics: [retrieval_f1]
          speed_threshold: 5
        modules:
          - module_type: pass_passage_filter
          - module_type: percentile_cutoff
            percentile: [0.4, 0.9]
          - module_type: similarity_threshold_cutoff
            threshold: [0.45, 0.95]
          - module_type: similarity_percentile_cutoff
            percentile: [0.4, 0.9]
      - node_type: passage_compressor
        strategy:
          metrics: [retrieval_token_f1, retrieval_token_recall, retrieval_token_precision]
          speed_threshold: 10
        modules:
          - module_type: pass_compressor
          - module_type: tree_summarize
            llm: openai
            model: gpt-3.5-turbo-16k
          - module_type: refine
            llm: openai
            model: gpt-3.5-turbo-16k
          - module_type: lexrank
            compression_ratio: [0.3, 0.7]
            threshold: [0.05, 0.3]
            damping: [0.75, 0.9]
            max_iterations: [15, 40]
          - module_type: spacy
            compression_ratio: [0.3, 0.5]
            spacy_model: ["en_core_web_sm", "en_core_web_md", "en_core_web_trf", "en_core_web_lg"]
  - node_line_name: post_retrieve_node_line
    nodes:
      - node_type: prompt_maker
        strategy:
          metrics:
            - metric_name: bleu
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai
          speed_threshold: 10
          generator_modules:
            - module_type: vllm
              llm: TinyLlama/TinyLlama-1.1B-Chat-v1.0
              temperature: [0.1, 1.0]
              max_tokens: 512
        modules:
          - module_type: fstring
            prompt:
              - "Answer to given questions with the following passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"
              - "There is a passages related to user question. Please response carefully to the following question. \n\n Passage: {retrieved_contents} \n\n Question: {query} \n\n Answer the question. Think step by step."
              - "{retrieved_contents} \n\n Read the passage carefully, and answer this question. \n\n Question: {query} \n\n Answer the question. Be concise."
          - module_type: long_context_reorder
            prompt:
              - "Answer to given questions with the following passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"
              - "There is a passages related to user question. Please response carefully to the following question. \n\n Passage: {retrieved_contents} \n\n Question: {query} \n\n Answer the question. Think step by step."
              - "{retrieved_contents} \n\n Read the passage carefully, and answer this question. \n\n Question: {query} \n\n Answer the question. Be concise."
          - module_type: window_replacement
            prompt:
              - "Tell me something about the question: {query} \n\n {retrieved_contents}"
              - "Question: {query} \n Something to read: {retrieved_contents} \n What's your answer?"
              - "Question: {query} \n\n Related information: \n{retrieved_contents} \n\n What would be a good response?"
      - node_type: generator
        strategy:
          metrics:
            - metric_name: bleu
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai
          speed_threshold: 10
        modules:
          - module_type: vllm
            llm: ["meta-llama/Llama-2-7b-chat-hf", "meta-llama/Llama-3.2-1B-Instruct", "microsoft/Phi-3-mini-4k-instruct", "Qwen/Qwen3-4B", "Qwen/Qwen2.5-1.5B-Instruct", "google/gemma-2b", "google/gemma-3-1b-it", "google/gemma-2-2b-it", "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "TheBloke/Llama-2-7B-Chat-AWQ", "TheBloke/Llama-2-13B-chat-AWQ", "TheBloke/CodeLlama-7B-Instruct-AWQ", "TinyLlama/TinyLlama-1.1B-Chat-v1.0"]
            temperature: [0.1, 1.0]
            max_tokens: 512